{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• _DeepFire_: API Project for Fire Detection üî•\n",
    "\n",
    "In this project, you'll apply your skills at neural network development in a new way: taking a model that you've trained yourself and deploying it to a static webpage that you can work with to upload new images and get prediction accuracy results. \n",
    "\n",
    "This project will primarily focus on your abilities in creating and testing neural network architecture development. \n",
    "\n",
    "Boilerplate and supporting architectures have been provided for a multitude of tasks ranging from data preprocessing, processing, ingestion, and predictive assessment ‚Äì¬†however, major tasks and design work will ultimately be left to you to approach and figure out ideal, optimized solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ General Importations\n",
    "\n",
    "As always, we'll start with importing basic tools and functions for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, PIL\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### üîé Initializing Deep Learning Tools üîç\n",
    "\n",
    "---\n",
    "\n",
    "Your first task will be crucial to ensuring the successful implementation of the rest of your notebook. \n",
    "\n",
    "**Initialize each line with the correct function type from the TensorFlow documentation.**\n",
    "\n",
    "Feel free to refer throughout the notebook and across previous notebooks to see which TensorFlow architectures you've used for similar tasks. \n",
    "\n",
    "To give you a guide for how this should look, you've been provided with a single correct function declaration in the form of `image_dataset_from_directory` at the end of the cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sequential Model Architecture \"\"\"\n",
    "# TODO: Initialize the sequential model architecture here.\n",
    "Sequential = tf.keras.models.Sequential\n",
    "\n",
    "\"\"\" Data Preprocessing Functions \"\"\"\n",
    "# TODO: Initialize the experimental resizing layer here.\n",
    "Resizing = tf.keras.layers.experimental.preprocessing.Resizing\n",
    "# TODO: Initialize the experimental rescaling layer here.\n",
    "Rescaling = tf.keras.layers.experimental.preprocessing.Rescaling\n",
    "\n",
    "\"\"\" Data Augmentation Functions \"\"\"\n",
    "# TODO: Initialize the experimental random flipping layer here.\n",
    "RandomFlip = tf.keras.layers.experimental.preprocessing.RandomFlip\n",
    "# TODO: Initialize the experimental random rotating layer here.\n",
    "RandomRotation = tf.keras.layers.experimental.preprocessing.RandomRotation\n",
    "# TODO: Initialize the experimental random zooming layer here.\n",
    "RandomZoom = tf.keras.layers.experimental.preprocessing.RandomZoom\n",
    "\n",
    "\"\"\" Artificial Neural Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the dense connective layer here.\n",
    "Dense = tf.keras.layers.Dense\n",
    "# TODO: Initialize the dropout regularization layer here.\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "\"\"\" Convolutional Neural Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the 2D convolutional layer here.\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "# TODO: Initialize the 2D max pooling layer here.\n",
    "MaxPool2D = tf.keras.layers.MaxPool2D\n",
    "# TODO: Initialize the flattening layer here.\n",
    "Flatten = tf.keras.layers.Flatten\n",
    "\n",
    "\"\"\" Residual Network Layer Inventory \"\"\"\n",
    "# TODO: Initialize the Residual Network multilayer model here.\n",
    "# TODO: Make sure you initialize the 50-layer residual network! \n",
    "# HINT: Look up `ResNet50` for appropriate documentation. \n",
    "ResNet50 = tf.keras.applications.resnet50.ResNet50\n",
    "\n",
    "\"\"\" Function to Load Images from Target Folder \"\"\"\n",
    "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ Precheck Image Dataset Sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fire image samples: 109\n",
      "Number of non-fire image samples: 532\n"
     ]
    }
   ],
   "source": [
    "# Use the `glob.glob` function to show how many images are in each folder\n",
    "DATA_DIRECTORY = \"../dataset/Images/\"\n",
    "FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Fire_Images/*\"\n",
    "NOT_FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Normal_Images/*\"\n",
    "\n",
    "print(f\"Number of fire image samples: {len(glob(FIRE_IMAGES_PATTERN))}\")\n",
    "print(f\"Number of non-fire image samples: {len(glob(NOT_FIRE_IMAGES_PATTERN))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 641 files belonging to 2 classes.\n",
      "Using 513 files for training.\n"
     ]
    }
   ],
   "source": [
    "train = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 641 files belonging to 2 classes.\n",
      "Using 128 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_performant_datasets(dataset, shuffling=None):\n",
    "    \"\"\" \n",
    "    Custom function to prefetch and cache stored elements\n",
    "    of retrieved image data to boost latency and performance\n",
    "    at the cost of higher memory usage. \n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    # Cache and prefetch elements of input data for boosted performance\n",
    "    if not shuffling:\n",
    "        return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        return dataset.cache().shuffle(shuffling).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =         configure_performant_datasets(train, shuffling=1000)\n",
    "validation =    configure_performant_datasets(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing_layer =        Resizing(IMAGE_HEIGHT,\n",
    "                                 IMAGE_WIDTH)\n",
    "\n",
    "normalization_layer =   Rescaling(1./255,\n",
    "                                  input_shape=(IMAGE_HEIGHT,\n",
    "                                               IMAGE_WIDTH,\n",
    "                                               3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
